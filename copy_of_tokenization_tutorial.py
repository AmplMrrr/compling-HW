# -*- coding: utf-8 -*-
"""Copy_of_tokenization_tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ehT_9uaaaxOowQBNPo9aZ6yIf5OGEVg9

# Токенизация в NLP

## Введение

**Токенизация** – это процесс разбиения текста на минимальные единицы (токены): слова, подслова или символы.  
Она является первым шагом во многих NLP задачах: анализ тональности, машинный перевод, чат-боты, обучение языковых моделей.

В этом туториале мы разберём разные виды токенизации и библиотеки, которые их реализуют.

## 1. Простые методы токенизации

### 1.1. Разделение по пробелам
"""

text = "Hello, world! This is a test."
text.split()

"""
### 1.2. Регулярные выражения
"""

import re

text = "Hello, world! This is a test."
tokens = re.findall(r"\w+", text)
tokens

"""**Ограничения простых методов**:  
- Не учитывают пунктуацию.  
- Не работают с разными языками и морфологией.  
- Не подходят для серьёзных задач.

## 2. Токенизация с помощью библиотек

### 2.1. NLTK  
**Применение**: образовательные проекты, простая предобработка текста.  
**Ограничения**: работает медленно на больших корпусах.  
**Когда использовать**: для курсовых и лабораторных работ по основам NLP.
"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize, sent_tokenize

text = "Hello, world! This is a test."
print(word_tokenize(text))
print(sent_tokenize(text))

"""
### 2.2. spaCy  
**Применение**: промышленные проекты, морфология, синтаксис.  
**Ограничения**: большие модели, требует ресурсов.  
**Когда использовать**: для курсовых по синтаксису и морфологии, проектов по обработке естественного языка.
"""

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Hello, world! This is a test.")
[t.text for t in doc]

"""
### 2.3. Stanza  
**Применение**: многоязычные корпуса, морфология, синтаксис.  
**Ограничения**: медленнее spaCy, требует загрузки моделей.  
**Когда использовать**: в исследованиях и при работе с редкими языками.
"""

!pip install stanza
import stanza

stanza.download("en")
nlp = stanza.Pipeline("en")
doc = nlp("Hello, world! This is a test.")
[[word.text for word in sent.words] for sent in doc.sentences]

"""
### 2.4. MosesTokenizer (sacremoses)  
**Применение**: машинный перевод, предобработка текстов.  
**Ограничения**: устаревающий стандарт.  
**Когда использовать**: для экспериментов с классическими MT-системами.
"""

!pip install sacremoses

from sacremoses import MosesTokenizer

mt = MosesTokenizer()
mt.tokenize("Hello, world! This is a test.")

"""
### 2.5. gensim  
**Применение**: подготовка данных для моделей `word2vec`, `fastText`.  
**Ограничения**: базовая токенизация.  
**Когда использовать**: при обучении собственных векторных моделей.  
"""

!pip install gensim
from gensim.utils import simple_preprocess

simple_preprocess("Hello, world! This is a test.")

"""
### 2.6. HuggingFace Tokenizers  
**Применение**: трансформеры, нейросетевые модели.  
**Ограничения**: сложность, нужен GPU для обучения токенизаторов.  
**Когда использовать**: в курсовых и проектах с BERT, GPT, T5.
"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer("Hello, world! This is a test.")
tokens

"""
## 3. Посимвольная токенизация  
"""

list("Hello, world!")

"""## 4. Комбинированные методы  

- **Послоговая токенизация** (актуально для языков с чёткой слоговой структурой, например, японский).  
- **Гибридные методы**: совмещение слов и символов.  

Пример: статья *Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models*.

## 5. Сравнение токенизаторов  

Попробуем применить разные методы к одному и тому же тексту.

## 6. Практика

Возьмём новостной текст, токенизируем разными способами и сравним.
"""

article = 'Natural language processing (NLP) is a field of artificial intelligence\
that gives computers the ability to understand text and spoken words in much the same way human beings can.'

print() #добавляю отбивку для наглядности (здесь и далее)
print("Текст для токенизация:", article) #вывожу тест для наглядности
print()
print("***"*20)
print()
print("Разные методы токенизации:") #для наглядности вывожу тест о том, что собираюсь дальше писать
print()
print("***"*20)
print()

"""
NLTK
"""
import nltk #имопртирую все необходимое (мы это делали выше, и можно было не повторять код, но если бы код до этого был пустой, эти действия были бы обязательны)
nltk.download('punkt')
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize, sent_tokenize

print("\033[1m" + "NLTK (по словам):" + "\033[0m", word_tokenize(article)) #делаю и вывожу токинизацию по словам и выделяю названин жирным для нагляжности
print("\033[1m" + "NLTK (по предложениям):" + "\033[0m", sent_tokenize(article)) #делаю и вывожу токинизацию по предложениями


"""
SpaCy
"""

import spacy #имопртирую все необходимое

nlp = spacy.load("en_core_web_sm")
doc = nlp(article)

print()
print("***"*20)
print()

print("\033[1m" + "SpaCy:" + "\033[0m", [t.text for t in doc]) #делаю и вывожу токинизацию способом SpaCy


"""
Gensim
"""
!pip install gensim
from gensim.utils import simple_preprocess

print()
print("***"*20)
print()

print("\033[1m" + "Gensim:" + "\033[0m", simple_preprocess(article)) #делаю и вывожу токинизацию способом Gensim

print()
print("***"*20)
print()
print("Сравнение:")
print()
print('Наиболее эффективным мне показался метод "Gensim", потому что он приводит все единицы к одному формату (нижний регистр) и не выделяет знаки преписания как токены.')
print('Остальные методы сохраняют изначальный вид слов (например, заглавные буквы) и выделяют знаки препинания.')
print('Выделение токенов с помощью метода "NLTK по предложениями" кажется мне самым неэффективным, хотя, вероятно, есть задачи для которых нужен именно он.')

"""## 7. Задача для самостоятельного разбора  

Прочитать статью:  
**"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"** (https://arxiv.org/abs/1604.00788)

### Напишите ответы на вопросы:
1. Что значит Out-of-Vocabulary?  
2. Как эту проблемы решили авторы статьи "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"?
3. Какие еще типы токенизации мы разбирали на занятии?

1. Термин "Out-of-Vocabulary" означает слова, которые встречаются в тексте (например в том, который нужно протокенизировать), но которых нет в словаре у модели, которая проводит токенизацию.
2. Чтобы решить проблему с Out-of-Vocabulary, авторы статьи предлагают проводить токенизацию двумя способами, то есть применять для таких единиц посимвольную токенизацию, а  остальные единицы токенизировать по словам.  В этом и состоит суть гибридной модели (выделять токены и на уровне слов, и на уровне символов).
3. Кроме токенизации по словам и символам на занятии мы говорили про существование таких типов токенизации, как: токенизации по предложениям, токенизация по N-граммам, подсловная токенизация (на паре был пример про выделение английского суффикса "-ly" как субтокена, ведь он является общим для большого количества слов).
"""