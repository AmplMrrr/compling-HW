{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmplMrrr/compling-HW/blob/main/hw_tokenization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "def var1_tokenization(string): #создаю функцию\n",
        "  import re #импоритирую re, потому что хочу работать через него\n",
        "  string = re.sub(r'[.?!,]', ' ', string) #меняю знаки препинания (список можно расширить, но для наших предложений и этого достаточно) на пробел\n",
        "  string = re.sub(r'\\s+', ' ', string) #убираю лишние пробелы\n",
        "  return string.split() #возвращаю результат в виде списка слов, поделенных по пробелам\n",
        "\n",
        "for el in text: #перебираю предложения из списка text\n",
        "  print(var1_tokenization(el)) #к каждому предложению применяю функцию и вывожу токенизированные предложения в виде списка слов\n",
        "\n",
        "#или можно проще\n",
        "print()\n",
        "\n",
        "def var11_tokenization(string):\n",
        "  import re\n",
        "  string = re.findall(r\"\\w+\", string)\n",
        "  return string\n",
        "\n",
        "for el in text:\n",
        "  print(var11_tokenization(el))\n",
        "\n",
        "#во втором варианте апостроф тоже считается знаком препинания, может так даже эффективнее"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1351efd-f115-4969-ca4c-33a106d2dbc5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', \"It's\", 'a', 'beautiful', 'day']\n",
            "['Dr', 'Smith', 'arrived', 'at', '5:30', 'p', 'm', 'from', 'New', 'York', 'The', 'meeting', 'cost', '$1', '000', '50']\n",
            "['I', \"can't\", 'believe', \"she's\", 'going', \"Let's\", 'meet', 'at', \"Jane's\", 'house', \"They'll\", 'love', 'it']\n",
            "[\"What's\", 'the', 'ETA', 'for', 'the', 'package', 'Please', 'e-mail', 'support@example', 'com', 'ASAP']\n",
            "\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'It', 's', 'a', 'beautiful', 'day']\n",
            "['Dr', 'Smith', 'arrived', 'at', '5', '30', 'p', 'm', 'from', 'New', 'York', 'The', 'meeting', 'cost', '1', '000', '50']\n",
            "['I', 'can', 't', 'believe', 'she', 's', 'going', 'Let', 's', 'meet', 'at', 'Jane', 's', 'house', 'They', 'll', 'love', 'it']\n",
            "['What', 's', 'the', 'ETA', 'for', 'the', 'package', 'Please', 'e', 'mail', 'support', 'example', 'com', 'ASAP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "def var2_tokenization(string): #создаю функцию\n",
        "  import nltk #импортирую все необходимое\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('punkt_tab')\n",
        "\n",
        "  from nltk.tokenize import word_tokenize #не стала импортировать sent_tokenize, потому что каждый элемент в листе text и так является предложением\n",
        "  return(word_tokenize(string)) #возвращаю токенизацию с помощью NLTK\n",
        "\n",
        "for el in text: #перебираю предложения из списка text\n",
        "  print(var2_tokenization(el)) #к каждому предложению применяю функцию и вывожу токенизированные предложения в виде списка слов"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d48affb-1acf-40e8-a20a-1287690dba90"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "def var3_tokenization(string): #создаю функцию\n",
        "  import spacy #импортирую все необходимое\n",
        "\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(string)\n",
        "  return[t.text for t in doc] #возвращаю результат работы функции в виже списка слов\n",
        "\n",
        "for el in text: #перебираю предложения из списка text\n",
        "  print(var3_tokenization(el)) #к каждому предложению применяю функцию и вывожу токенизированные предложения в виде списка слов"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c63eb0-27a7-4f2c-f8a4-b7b2516a918f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf104ec-6295-4765-b259-46c573523dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация для предложения: The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "Токенизация по пробелам и знакам препинания (вариант 1): ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', \"It's\", 'a', 'beautiful', 'day']\n",
            "Токенизация по пробелам и знакам препинания (вариант 2): ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'It', 's', 'a', 'beautiful', 'day']\n",
            "Токенизация NLTK: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация SpaCy: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "************************************************************\n",
            "\n",
            "Токенизация для предложения: Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\n",
            "Токенизация по пробелам и знакам препинания (вариант 1): ['Dr', 'Smith', 'arrived', 'at', '5:30', 'p', 'm', 'from', 'New', 'York', 'The', 'meeting', 'cost', '$1', '000', '50']\n",
            "Токенизация по пробелам и знакам препинания (вариант 2): ['Dr', 'Smith', 'arrived', 'at', '5', '30', 'p', 'm', 'from', 'New', 'York', 'The', 'meeting', 'cost', '1', '000', '50']\n",
            "Токенизация NLTK: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация SpaCy: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "\n",
            "************************************************************\n",
            "\n",
            "Токенизация для предложения: I can't believe she's going! Let's meet at Jane's house. They'll love it.\n",
            "Токенизация по пробелам и знакам препинания (вариант 1): ['I', \"can't\", 'believe', \"she's\", 'going', \"Let's\", 'meet', 'at', \"Jane's\", 'house', \"They'll\", 'love', 'it']\n",
            "Токенизация по пробелам и знакам препинания (вариант 2): ['I', 'can', 't', 'believe', 'she', 's', 'going', 'Let', 's', 'meet', 'at', 'Jane', 's', 'house', 'They', 'll', 'love', 'it']\n",
            "Токенизация NLTK: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация SpaCy: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "\n",
            "************************************************************\n",
            "\n",
            "Токенизация для предложения: What's the ETA for the package? Please e-mail support@example.com ASAP!\n",
            "Токенизация по пробелам и знакам препинания (вариант 1): [\"What's\", 'the', 'ETA', 'for', 'the', 'package', 'Please', 'e-mail', 'support@example', 'com', 'ASAP']\n",
            "Токенизация по пробелам и знакам препинания (вариант 2): ['What', 's', 'the', 'ETA', 'for', 'the', 'package', 'Please', 'e', 'mail', 'support', 'example', 'com', 'ASAP']\n",
            "Токенизация NLTK: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация SpaCy: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n",
            "\n",
            "************************************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ваш код здесь\n",
        "for el in text: #перебираю предложения в списке\n",
        "  print(f\"Токенизация для предложения: {el}\") #вывожу само предложение\n",
        "  print(\"Токенизация по пробелам и знакам препинания (вариант 1):\", var1_tokenization(el)) #вывожу результат применения первой функции (и дальше по аналогии)\n",
        "  print(\"Токенизация по пробелам и знакам препинания (вариант 2):\", var11_tokenization(el))\n",
        "  print(\"Токенизация NLTK:\", var2_tokenization(el))\n",
        "  print(\"Токенизация SpaCy:\",var3_tokenization(el))\n",
        "  print() #делаю отбивку предложений для наглядности\n",
        "  print(\"***\"*20)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Деление по пробелам и знакам препинания может быть недостаточным, потому что существуют разные языки (со своими особенностями) и разные цели, для которых проводится токенизация.\n",
        "Например, некоторые азиатские языки (китайский, японский) не имеют пробелов, для их обработки токенизация по пробелам не подходит.\n",
        "Также есть языки со сложной структрурой (русский, чешский), для которых эффективнее будет подсловная токенизация, ведь большую роль в этих языках играют, например, окончения. Если мы будем токенизировать тексты на этих языках по пробелам, то получим много токенов, которые, на самом деле, являются одним словом, но в разных падежах/родах.\n",
        "Что касается разных целей, для которых проводится токенизация, то можно сказать про исследования больших текстовых данных с целью выявления устоявщихся словосочетаний. Если мы в исследовательском проекте хотим посмотреть на частоту употребления устоявшейся языковой единицы (например, фразеологизма \"бить баклуши\") в каком-либо массиве текстов, токенизация по пробелам или по знакам препинания нам не совсем подходит. Нужна будет модель, в словаре которой этот фразеологизм будет считаться самостоятельным токеном.\n",
        "\n",
        "2. В этой фразе в модели GPT-5 10 токенов. Это значение я получила, написав эту фразу в gpt-tokinizer. Ссылка: https://gpt-tokenizer.dev/\n",
        "\n",
        "3. BPE - это подсловная токенизация. В рамках этого подхода токенами считаются не только отдельные слова, но и части слов (например, суффиксы), которые часто повторяются в большом количество слов. Эти части слов называются \"субтокенами\".\n",
        "Если говорить технически, то в модель загружают массив данных. Слова, загруженные в модель, представляют собой набор токенов (последовательность символов). Модель смотрит, какие токены встречаются вместе чаще всего (то есть, говоря очень примитивно, какие буквы в слове часто стоят рядом, и этот паттерн повторяется во многих словах) и, находя такое сочетание токенов, модель \"записывает\" его как новый токен (например, токен \"l\" + токен \"y\" = токен \"ly\". Мы знаем, что это английский суффикс, позволяющий сделать из прилагательного наречие, но для модели это просто паттерн, который надо \"найти\"). Модель продолжает поиск часто встречающихся токенов, пока не достигнет ограничения словаря."
      ],
      "metadata": {
        "id": "-sEfvrKzVsDR"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}